---
title: "Squarespace Sales Site Scraping Tool (Python)"
author: "Khizer Asad"
date: 2020-10-12T13:09:13-06:00
categories: ["Python"]
output: html_document
---

# Introduction 

I was recently tasked with extracting a list of products sold on hotel supply websites. A recurring pattern I noticed in these various sites was they were mostly made using website building services, the most prevalent being Squarespace. This streamlined my job as it meant that the construction of the varying sites would be very similar, so if I wanted to write a web scraping tool in Python to extract all the products they sell I could use the same code with minimal changes. For this example I'll be using the website https://www.truenorthdistributors.com/ .

TL;DR you can find the code on my github [here](https://github.com/KhizerA/Python-Web-Scraping-Tool/blob/main/Scrapping%20tool.py)

# The Code

The starting point of course was first getting the links for all products. There are already many examples of this online so I just used [this guide](https://www.thepythoncode.com/article/extract-all-website-links-python), and wrote all the links to a csv file. Now you'll have a list of ALL webpages of your desired website, which will likely include non-product links as well. Narrowing this down to just product pages is pretty simple, just find a common phrase in only the product links and filter by that. In the case of True North they have a common link for all products depending on the category it's in (e.g. /hotel-supplies/ or /hospitality-supplies). 

Now we have a csv file with the links to all products we can get going. Using the help of [another guide](https://realpython.com/beautiful-soup-web-scraper-python/) I produced the code you'll find below:

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, eval=FALSE}
import requests
from bs4 import BeautifulSoup
import pandas
import sys

file = pandas.read_csv('[Insert links csv file path]')
link_list = file['Link'].tolist()
master_l = []
master_p = []
master_d = []

for index, links in enumerate(link_list):
    product_list = []
    description_list = []
    URL = links
    page = requests.get(URL)
    soup = BeautifulSoup(page.content, 'html.parser')
    results = soup.find(id='content')
    if results is None:
        continue
    job_elems = results.find_all('div', class_='sqs-block html-block sqs-block-html')

    for num, job_elem in enumerate(job_elems):
        list = []
        title = job_elem.find('h2')
        description = job_elem.find_all('p', class_='')
        if title is None:
            continue
        product_list.insert(num, title.get_text())

        for i, x in enumerate(description):
            if description[i] is None:
                continue
            list.insert(i, description[i].text)
        des_output = ', '.join(list)
        description_list.insert(num, des_output)

    master_p.extend(product_list)
    master_d.extend(description_list)
    for count in range(len(product_list)):
        master_l.append(links)

dictionary = {'Link': master_l, 'Product': master_p, 'Description': master_d}
df = pandas.DataFrame(dictionary)
df.to_csv('[Insert csv file path]')

```

This will leave you with a csv file containing all product names, descriptions, and its respective link. Depending on the website you run this on, you will need to inspect the HTML of one of the product webpages and change the Beautiful Soup functions filters to make sure you catch the information you need. 
